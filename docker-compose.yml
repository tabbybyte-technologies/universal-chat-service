services:
  api:
    profiles: ["docker-llama"]
    build:
      context: .
      dockerfile: Dockerfile
    container_name: universal-chat-api
    environment:
      PORT: ${PORT:-3000}
      API_KEY: ${API_KEY:-change-me}
      LLAMA_BASE_URL: ${LLAMA_BASE_URL_DOCKER:-http://llama-cpp:8080/v1}
      LLAMA_MODEL: ${LLAMA_MODEL:-local-model}
      LLAMA_API_KEY: ${LLAMA_API_KEY:-dummy}
    ports:
      - "${PORT:-3030}:3000"
    depends_on:
      llama-cpp:
        condition: service_healthy
      redis:
        condition: service_healthy

  api-host:
    profiles: ["host-llama"]
    build:
      context: .
      dockerfile: Dockerfile
    container_name: universal-chat-api-host
    environment:
      PORT: ${PORT:-3000}
      API_KEY: ${API_KEY:-change-me}
      LLAMA_BASE_URL: ${LLAMA_BASE_URL_HOST:-http://host.docker.internal:8080/v1}
      LLAMA_MODEL: ${LLAMA_MODEL:-local-model}
      LLAMA_API_KEY: ${LLAMA_API_KEY:-dummy}
    ports:
      - "${PORT:-3030}:3000"
    depends_on:
      redis:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"

  llama-cpp:
    profiles: ["docker-llama"]
    image: ${LLAMA_IMAGE:-ghcr.io/ggml-org/llama.cpp:server}
    platform: ${LLAMA_PLATFORM:-linux/amd64}
    container_name: universal-llama-cpp
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        MODEL_PATH="/models/${LLAMA_MODEL_FILE:-model.gguf}";
        if [ ! -f "$$MODEL_PATH" ]; then
          echo "ERROR: Missing GGUF model file at $$MODEL_PATH";
          echo "Place your model in ./models and set LLAMA_MODEL_FILE if needed.";
          exit 1;
        fi;
        exec /app/llama-server \
          --host 0.0.0.0 \
          --port 8080 \
          --model "$$MODEL_PATH" \
          --ctx-size ${LLAMA_CTX_SIZE:-4096} \
          --parallel ${LLAMA_PARALLEL:-1}
    volumes:
      - ./models:/models:ro
    expose:
      - "8080"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "(wget -q -O - http://127.0.0.1:8080/health || curl -fsS http://127.0.0.1:8080/health) >/dev/null 2>&1 || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s

  redis:
    image: redis:7-alpine
    container_name: universal-redis
    command: ["redis-server", "--appendonly", "no"]
    expose:
      - "6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 10
